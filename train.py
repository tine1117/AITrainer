import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, AutoConfig
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from data import data
import argparse
import utils

# ì¥ì¹˜ ì„¤ì • (ë§¥ ìš© MPS í˜¹ì€ Nvidia GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì‚¬ìš©)
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"

# yaml íŒŒì§•
def parse_args(args=None, namespace=None):
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        required=True,
        help="path to the config file")
    return parser.parse_args(args=args, namespace=namespace)

# parse commands
cmd = parse_args()
# load config
args = utils.load_config(cmd.config)

# Checkpoint ì°¾ê¸° í•¨ìˆ˜
def get_last_checkpoint(output_dir):
    if not os.path.exists(output_dir):
        return None
    checkpoints = [
        os.path.join(output_dir, d)
        for d in os.listdir(output_dir)
        if d.startswith("checkpoint-")
    ]
    if not checkpoints:
        return None
    # ìµœì‹  checkpoint ë¦¬í„´
    return max(checkpoints, key=lambda x: int(x.split('-')[-1]))

def numcheck(checkpoint_name):
    checkpoint = os.path.basename(checkpoint_name)
    if not checkpoint.startswith("checkpoint-"):
        # í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜
        return 0
    try:
        # "checkpoint-" ì´í›„ì˜ ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ
        number = int(checkpoint.split('-')[-1])
        return number
    except ValueError:
        # ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê²½ìš° None ë°˜í™˜
        return 0

def inputepoch(num, last_checked):
    print(f"í•™ìŠµë  epoch ê¸°ë³¸ê°’ : {num}");
    print(f"í˜„ì¬ í•™ìŠµëœ checkpoint ê°’ : {last_checked}");
    input_num = input("ë” í•™ìŠµí•  epoch ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ")
    # ì—”í„°ì‹œ
    if input_num == '':
        print("epoch ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return num + last_checked
    # ë¬¸ìì¼ ê²½ìš°
    elif not input_num.isdigit():
        print("ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ì„¤ì •í•œ epoch(250) ë§Œí¼ í•™ìŠµí•œ ë‹¤ìŒ ì¢…ë£Œ í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”")
        return last_checked
    # ìˆ«ìì¼ ê²½ìš°
    else :
        sum = int(input_num) + last_checked
        # í˜¹ì‹œëª¨ë¥¼ í˜„ë³€í™˜
        sum = int(sum)
        print(f"{int(input_num)} epoch ê°’ ë§Œí¼ ì¶”ê°€ í•™ìŠµí•©ë‹ˆë‹¤.")
        return sum

# OUTPUT_DIRê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(args.train.output_dir, exist_ok=True)

# ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
last_checkpoint = get_last_checkpoint(args.train.output_dir)

if last_checkpoint == None:
    print("âœ… ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    num_of_epoch=5
else:
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {last_checkpoint}. ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")


# ëª¨ë¸ ë¡œë“œ
if last_checkpoint:
    print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(last_checkpoint).to(DEVICE)
    last_epoch = numcheck(last_checkpoint)
    num_of_epoch = inputepoch(250, last_epoch)
else:
    print("ğŸŒ± ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(args.train.MODEL_NAME).to(DEVICE)

# LoRA ì„¤ì • ë° ëª¨ë¸ ì ìš©
lora_config = LoraConfig(
    r=16,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none"
)
model = get_peft_model(model, lora_config)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(args.train.MODEL_NAME)

# ë°ì´í„°ì…‹ êµ¬ì„±
dataset = Dataset.from_list(data)
split_data = dataset.train_test_split(test_size=0.0003)

def tokenize_function(example):
    prompt = example["instruction"] + " " + example["input"]
    inputs = tokenizer(prompt, truncation=True, padding="max_length", max_length=128)
    labels = tokenizer(example["output"], truncation=True, padding="max_length", max_length=128)
    inputs["labels"] = labels["input_ids"]
    return inputs

tokenized_train_dataset = split_data["train"].map(tokenize_function)
tokenized_val_dataset = split_data["test"].map(tokenize_function)

# í‰ê°€ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    loss_fn = torch.nn.CrossEntropyLoss()
    logits = torch.tensor(logits).to(DEVICE)
    labels = torch.tensor(labels).to(DEVICE)
    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
    return {"eval_loss": loss.item()}

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir=args.train.OUTPUT_DIR,
    per_device_train_batch_size=args.train.batch_size,
    num_train_epochs=int(num_of_epoch),
    logging_steps=args.train.logging_steps,
    save_steps=args.train.save_steps,
    save_total_limit=args.train.save_total_limit,
    learning_rate=args.train.lr,
    fp16=args.train.fp16,  # ë§¥ í˜¹ì€ cpuìš©ìœ¼ë¡œ ëŒë¦´ë ¤ë©´ False í˜¹ì€ ì£¼ì„ í•„ìš”
    gradient_accumulation_steps=args.train.gradient_accumulation_steps,
    metric_for_best_model="eval_steps_per_second",
    warmup_ratio=args.train.warmup_ratio,
    eval_steps=args.train.eval_steps,
    evaluation_strategy="steps",
    save_strategy="steps",
    load_best_model_at_end=True,
)

# Trainer êµ¬ì„±
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    compute_metrics=compute_metrics,
)

# í•™ìŠµ ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ)
trainer.train(resume_from_checkpoint=last_checkpoint)

# ëª¨ë¸ ì €ì¥
model.save_pretrained(args.train.LOCAL_MODEL_PATH)
tokenizer.save_pretrained(args.train.LOCAL_MODEL_PATH)

print("ğŸ‰ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")